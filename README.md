# YOLO-V5 GRADCAM

I constantly desired to know to which part of an object the object-detection models pay more attention. So I searched for it, but I didn't find any for Yolov5.
Here is my implementation of Grad-cam for YOLO-v5. To load the model I used the yolov5's main codes, and for computing GradCam I used the codes from the gradcam_plus_plus-pytorch repository.
Please follow my GitHub account and star â­ the project if this functionality benefits your research or projects.
light-toned wood, likely a natural or lightly stained wood species, top-down view, overhead perspective, flat angle, clear wood grain texture, realistic lighting, high detail


wall with wallpaper only, front view, flat angle, light-toned wallpaper, photo-realistic, high resolution  
Negative prompt: floor, ceiling, furniture, window, door, people, clutter

## Update:
Repo works fine with yolov5-v6.1


## Installation
`pip install -r requirements.txt`

## Infer
`python main.py --model-path yolov5s.pt --img-path images/cat-dog.jpg --output-dir outputs`

**NOTE**: If you don't have any weights and just want to test, don't change the model-path argument. The yolov5s model will be automatically downloaded thanks to the download function from yolov5. 

**NOTE**: For more input arguments, check out the main.py or run the following command:

```python main.py -h```

### Custom Name
To pass in your custom model you might want to pass in your custom names as well, which be done as below:
```
python main.py --model-path cutom-model-path.pt --img-path img-path.jpg --output-dir outputs --names obj1,obj2,obj3 
```
## Examples
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pooya-mohammadi/yolov5-gradcam/blob/master/main.ipynb)

<img src="https://raw.githubusercontent.com/pooya-mohammadi/yolov5-gradcam/master/outputs/eagle-res.jpg" alt="cat&dog" height="300" width="1200">
<img src="https://raw.githubusercontent.com/pooya-mohammadi/yolov5-gradcam/master/outputs/cat-dog-res.jpg" alt="cat&dog" height="300" width="1200">
<img src="https://raw.githubusercontent.com/pooya-mohammadi/yolov5-gradcam/master/outputs/dog-res.jpg" alt="cat&dog" height="300" width="1200">

## Note
I checked the code, but I couldn't find an explanation for why the truck's heatmap does not show anything. Please inform me or create a pull request if you find the reason.

This problem is solved in version 6.1

Solve the custom dataset gradient not match.

# References
```python
import os
from typing import TypedDict, Annotated
from pydantic import Field
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

# ==========================================
# 0. ç¯å¢ƒå‡†å¤‡
# ==========================================
# è¯·ç¡®ä¿è®¾ç½®äº†ä½ çš„ OpenAI API Keyã€‚
# å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…¶ä»–å…¼å®¹ OpenAI æ¥å£çš„æ¨¡å‹ï¼ˆå¦‚ DeepSeek, Qwenï¼‰ï¼Œè¯·ä¿®æ”¹ base_url å’Œæ¨¡å‹åç§°ã€‚
os.environ["OPENAI_API_KEY"] = "your-openai-api-key-here" 

# ==========================================
# 1. å®šä¹‰åŠ¨ä½œå·¥å…· (Tools)
# ==========================================
@tool
def call_phone(target_role: str = Field(description="The role or person to call, e.g., 'admin', 'police'"), 
               reason: str = Field(description="The reason for calling")):
    """Use this tool to make a phone call to a specific role."""
    print(f"\nğŸ“ [Tool Executed] Calling {target_role}... Reason: {reason}")
    return f"Successfully called {target_role}"

@tool
def send_sms(target_role: str = Field(description="The role or person to text, e.g., 'car owner'"), 
             message: str = Field(description="The text message content")):
    """Use this tool to send an SMS text message."""
    print(f"\nâœ‰ï¸ [Tool Executed] Sending SMS to {target_role}... Message: {message}")
    return f"SMS sent to {target_role}"

@tool
def trigger_alarm(floor: int = Field(description="The floor number to trigger the alarm on"), 
                  alarm_type: str = Field(description="Type of alarm, e.g., 'fire', 'intruder'")):
    """Use this tool to sound the physical building alarm."""
    print(f"\nğŸš¨ [Tool Executed] Triggering {alarm_type} alarm on floor {floor}!")
    return f"Alarm triggered on floor {floor}"

# å°†æ‰€æœ‰å¯ç”¨å·¥å…·æ‰“åŒ…
action_tools = [call_phone, send_sms, trigger_alarm]

# ==========================================
# 2. å®šä¹‰å›¾çŠ¶æ€ (State)
# ==========================================
class InspectionState(TypedDict):
    floor: int
    condition: str             # è§¦å‘æ¡ä»¶ (ä¾‹å¦‚: "detect human presence")
    action_rule: str           # è§¦å‘åçš„åŠ¨ä½œè§„åˆ™ (ä¾‹å¦‚: "call the floor administrator")
    camera_data: str           # æ¨¡æ‹Ÿçš„æ‘„åƒå¤´æ–‡å­—æè¿°ï¼ˆçœŸå®åœºæ™¯ä¸­å¯ä»¥æ˜¯å›¾ç‰‡çš„ Base64ï¼‰
    should_alert: bool         # å†…éƒ¨çŠ¶æ€ï¼šæ˜¯å¦éœ€è¦æŠ¥è­¦
    messages: Annotated[list, add_messages] # ç”¨äºå­˜å‚¨ Agent å’Œ Tool ä¹‹é—´çš„å¯¹è¯å†å²

# ==========================================
# 3. å®šä¹‰å›¾çš„èŠ‚ç‚¹ (Nodes)
# ==========================================
def analyze_node(state: InspectionState):
    """
    èŠ‚ç‚¹ 1ï¼šè´Ÿè´£â€œçœ‹â€ã€‚æ ¹æ®ä¼ å…¥çš„ condition åˆ¤æ–­å½“å‰ç”»é¢æ˜¯å¦å¼‚å¸¸ã€‚
    """
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    # çº¯è‹±æ–‡ Promptï¼Œä¿è¯é€»è¾‘ä¸¥å¯†æ€§
    sys_prompt = f"""You are an advanced security visual analysis agent.
Your task is to observe the provided surveillance data and determine if it triggers a specific condition.

[Trigger Condition]: {state['condition']}

Rules:
1. Carefully compare the visual data against the [Trigger Condition].
2. If the visual data STRICTLY MATCHES the condition, reply with exactly "YES".
3. If it does not match, or the scene is normal, reply with exactly "NO".
4. Do not output any additional explanation, formatting, or punctuation."""

    messages = [
        SystemMessage(content=sys_prompt),
        HumanMessage(content=f"Surveillance Data: {state['camera_data']}")
    ]
    
    response = llm.invoke(messages).content.strip().upper()
    should_alert = "YES" in response
    
    print(f"ğŸ‘€ [Analyze Node] Floor: {state['floor']} | Condition: '{state['condition']}'")
    print(f"   -> Model Judgment: {response} | Trigger Alert: {should_alert}")
    
    return {"should_alert": should_alert}


def action_agent_node(state: InspectionState):
    """
    èŠ‚ç‚¹ 2ï¼šè´Ÿè´£â€œå†³ç­–â€ã€‚å½“å‘ç°å¼‚å¸¸æ—¶ï¼Œæ ¹æ® action_rule é€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶ç”Ÿæˆè°ƒç”¨å‚æ•°ã€‚
    """
    llm_with_tools = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools(action_tools)
    
    sys_prompt = f"""You are a security action execution agent.
An anomaly has just been confirmed on floor {state['floor']}. 
The detected situation matches the condition: {state['condition']}.

[Required Action Rule]: {state['action_rule']}

Your task:
Based on the [Required Action Rule], select the most appropriate tool to execute the action.
Extract the necessary parameters (like target role, reason, or message) from the context.
Execute the tool immediately. Do not ask for user confirmation."""

    messages = [SystemMessage(content=sys_prompt)]
    
    print(f"ğŸ§  [Action Agent] Deciding action based on rule: '{state['action_rule']}'...")
    response = llm_with_tools.invoke(messages)
    
    return {"messages": [response]}


def alert_router(state: InspectionState):
    """
    æ¡ä»¶è·¯ç”±ï¼šæ ¹æ® analyze_node çš„ç»“æœå†³å®šå»å‘ã€‚
    """
    if state.get("should_alert"):
        return "action_agent"
    return END

# ==========================================
# 4. ç»„è£… LangGraph å·¥ä½œæµ
# ==========================================
workflow = StateGraph(InspectionState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("analyze", analyze_node)
workflow.add_node("action_agent", action_agent_node)
# ToolNode æ˜¯ LangGraph å†…ç½®çš„ä¸“é—¨ç”¨äºæ‰§è¡Œå¤§æ¨¡å‹è¾“å‡ºçš„å·¥å…·è°ƒç”¨çš„èŠ‚ç‚¹
workflow.add_node("execute_tools", ToolNode(action_tools)) 

# è®¾ç½®èµ·ç‚¹
workflow.set_entry_point("analyze")

# è®¾ç½®è¾¹ä¸è·¯ç”±
# 1. åˆ†æå®Œåï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦åŠ¨ä½œ
workflow.add_conditional_edges(
    "analyze", 
    alert_router, 
    {"action_agent": "action_agent", END: END}
)

# 2. åŠ¨ä½œå†³ç­–å®Œåï¼Œæ‰§è¡Œå·¥å…· (tools_condition ä¼šæ£€æŸ¥ LLM æ˜¯å¦çœŸçš„è¯·æ±‚äº†å·¥å…·)
workflow.add_conditional_edges(
    "action_agent", 
    tools_condition, 
    {"tools": "execute_tools", END: END}
)

# 3. å·¥å…·æ‰§è¡Œå®Œæ¯•åï¼Œæµç¨‹ç»“æŸ
workflow.add_edge("execute_tools", END)

# ç¼–è¯‘å›¾
app = workflow.compile()

# ==========================================
# 5. æµ‹è¯•ç”¨ä¾‹è¿è¡Œ
# ==========================================
if __name__ == "__main__":
    print("==================================================")
    print("TEST CASE 1: å‘ç°å¯ç–‘äººå‘˜ï¼Œè§¦å‘æ‹¨æ‰“ç”µè¯ (Should Call)")
    print("==================================================")
    state_1 = {
        "floor": 6,
        "condition": "detect human presence",
        "action_rule": "call the floor administrator",
        "camera_data": "A person in a black hoodie is walking down the hallway.",
        "messages": []
    }
    app.invoke(state_1)
    
    print("\n==================================================")
    print("TEST CASE 2: å‘ç°è¿åœï¼Œè§¦å‘å‘é€çŸ­ä¿¡ (Should SMS)")
    print("==================================================")
    state_2 = {
        "floor": -1,
        "condition": "detect a car parked outside of the designated parking lines",
        "action_rule": "send an SMS to the car owner asking them to move the car",
        "camera_data": "A red sedan is parked blocking the fire exit.",
        "messages": []
    }
    app.invoke(state_2)

    print("\n==================================================")
    print("TEST CASE 3: ç”»é¢æ­£å¸¸ï¼Œä¸è§¦å‘ä»»ä½•åŠ¨ä½œ (Should Ignore)")
    print("==================================================")
    state_3 = {
        "floor": 3,
        "condition": "detect fire or smoke",
        "action_rule": "trigger the fire alarm",
        "camera_data": "The server room is dark and quiet, all indicator lights are normal.",
        "messages": []
    }
    app.invoke(state_3)
import os
import cv2
import base64
from typing import TypedDict, List, Literal
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage


class AnswerItem(BaseModel):
    question_id: int = Field(description="é—®é¢˜çš„åºå·ï¼Œä» 1 å¼€å§‹")
    answer: Literal["yes", "no"] = Field(description="è¯¥é—®é¢˜çš„ç­”æ¡ˆï¼Œåªèƒ½æ˜¯ yes æˆ– no")

class VideoQAResponse(BaseModel):
    results: list[AnswerItem] = Field(description="è§†é¢‘é—®ç­”ç»“æœçš„åˆ—è¡¨")


class GraphState(TypedDict):
    video_path: str             # æœ¬åœ°è§†é¢‘æ–‡ä»¶çš„è·¯å¾„
    questions: List[str]        # æå‡ºçš„é—®é¢˜åˆ—è¡¨
    final_result: VideoQAResponse


def extract_frames_as_base64(video_path: str, interval_sec: int = 1, max_frames: int = 30) -> list[str]:
    """
    ä½¿ç”¨ OpenCV è¯»å–è§†é¢‘ï¼Œæ¯éš” interval_sec ç§’æŠ½å–ä¸€å¸§ï¼Œå¹¶è½¬æ¢ä¸º base64 å­—ç¬¦ä¸²ã€‚
    max_frames ç”¨äºä¿æŠ¤ API è°ƒç”¨ï¼Œé¿å…ä¼ å…¥è¶…é•¿è§†é¢‘å¯¼è‡´æ˜¾å­˜æº¢å‡ºæˆ–è´¹ç”¨è¿‡é«˜ã€‚
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    
    fps = round(cap.get(cv2.CAP_PROP_FPS))
    if fps == 0:
        fps = 30 
        
    frame_interval = int(fps * interval_sec) 
    
    base64_frames = []
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break 
            
        if frame_count % frame_interval == 0:
            _, buffer = cv2.imencode('.jpg', frame)
            b64_str = base64.b64encode(buffer).decode('utf-8')
            base64_frames.append(b64_str)
            
            if len(base64_frames) >= max_frames:
                print(f"[è­¦å‘Š] è¾¾åˆ°æœ€å¤§é™åˆ¶ {max_frames} å¸§ï¼Œå·²åœæ­¢ç»§ç»­æŠ½å¸§ã€‚")
                break
                
        frame_count += 1
        
    cap.release()
    print(f"[ä¿¡æ¯] æŠ½å¸§å®Œæˆï¼Œå…±æå– {len(base64_frames)} å¸§å›¾ç‰‡ã€‚")
    return base64_frames

def analyze_video_node(state: GraphState):

    llm = ChatOpenAI(
        temperature=0.1
    )
    
    structured_llm = llm.with_structured_output(VideoQAResponse)


    print(f"[å¤„ç†ä¸­] æ­£åœ¨ä» {state['video_path']} æå–è§†é¢‘å¸§...")
    base64_frames = extract_frames_as_base64(state["video_path"], interval_sec=1)
    

    questions_text = "\n".join([
        f"{i+1}. {q}" for i, q in enumerate(state["questions"])
    ])


    user_content = [
        {
            "type": "text", 
            "text": f"è¿™æ˜¯ä¸€æ®µè§†é¢‘æŒ‰1ç§’1å¸§æå–çš„è¿ç»­ç”»é¢ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™äº›ç”»é¢ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼ä»¥ yes æˆ– no å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n\n{questions_text}"
        }
    ]

    for b64_str in base64_frames:
        user_content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{b64_str}"}
        })

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„å¤šæ¨¡æ€è§†é¢‘åˆ†æå¼•æ“ã€‚å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æä¾›çš„ JSON Schema è¾“å‡ºç»“æœï¼Œä¸è¦è¾“å‡ºä»»ä½•å¤šä½™çš„è§£é‡Šã€‚"),
        HumanMessage(content=user_content)
    ]
    
    print("[å¤„ç†ä¸­] æ­£åœ¨è°ƒç”¨ Qwen-VL è¿›è¡Œæ¨ç† (è¿™å¯èƒ½éœ€è¦åå‡ ç§’)...")
    response = structured_llm.invoke(messages)
    
    return {"final_result": response}

workflow = StateGraph(GraphState)
workflow.add_node("analyze_node", analyze_video_node)
workflow.add_edge(START, "analyze_node")
workflow.add_edge("analyze_node", END)
app = workflow.compile()

if __name__ == "__main__":
    test_video_path = "1.mp4" 
    
    if not os.path.exists(test_video_path):
        print(f"æ‰¾ä¸åˆ°æµ‹è¯•è§†é¢‘: {test_video_path}ï¼Œè¯·æ”¾å…¥ä¸€ä¸ªè§†é¢‘æ–‡ä»¶åé‡è¯•ã€‚")
    else:
        inputs = {
            "video_path": test_video_path,
            "questions": [
                "è§†é¢‘ä¸­æ˜¯å¦å‡ºç°äº†äººç±»ï¼Ÿ",
                "è§†é¢‘çš„åœºæ™¯æ˜¯åœ¨å®¤å¤–å—ï¼Ÿ",
                "è§†é¢‘ä¸­æ˜¯å¦å‡ºç°äº†æ±½è½¦ï¼Ÿ",
                "ç”»é¢ä¸­æœ‰äººåœ¨è·‘æ­¥å—ï¼Ÿ"
            ]
        }

        result_state = app.invoke(inputs)
        final_answers = result_state["final_result"]
        
        print("\n=== âœ¨ æœ€ç»ˆåˆ†æç»“æœ ===")
        for item in final_answers.results:
            print(f"é—®é¢˜ {item.question_id}: {item.answer}")

